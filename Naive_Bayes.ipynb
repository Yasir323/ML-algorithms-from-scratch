{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Naive Bayes.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyOvzgZRclLgxIEc3ZsgpKzw",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Yasir323/ML-algorithms-from-scratch/blob/main/Naive_Bayes.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AlQ2NnxLMnoF"
      },
      "source": [
        "## Some basic understanding\n",
        "\n",
        "The key to Naive Bayes is making the (big) assumption that the presences (or absences) of each word are independent of one another, conditional on a message being spam or not. Intuitively, this assumption means that knowing whether a certain spam message contains the word bitcoin gives you no information about whether that same message contains the word rolex. In math terms, this means that:\n",
        "\n",
        "$ P(X_1=x_1,..., X_n=x_n|S) = P(X_1=x_1|S)*...*P(X_n=x_n|S) $\n",
        "\n",
        "This is an extreme assumption. (There’s a reason the technique has naive in its name.) Imagine that our vocabulary consists only of the words bitcoin and rolex, and that half of all spam messages are for “earn bitcoin” and that the other half are for “authentic rolex.” In this case, the Naive Bayes estimate that a spam message contains both bitcoin and rolex is:\n",
        "\n",
        "$P (X_1 = 1, X_2 = 1|S) = P (X_1 = 1|S) P (X_2 = 1|S) =0.5 × 0.5 = 0.25 $\n",
        "\n",
        "since we’ve assumed away the knowledge that bitcoin and rolex actually never occur together. Despite the unrealisticness of this assumption, this model often performs well and has historically been used in actual spam filters.\n",
        "\n",
        "In practice, you usually want to avoid multiplying lots of probabilities together, to prevent a problem called underflow, in which computers don’t deal well with floating-point numbers that are too close to 0. Recalling from algebra that $log(ab) = log a + log b and that exp(log x) = x $, we usually compute $p_1 * ⋯ * p_n $ as the equivalent (but floating-point-friendlier):\n",
        "\n",
        "$ exp(log (p_1) + ⋯ + log (p_n)) $\n",
        "\n",
        "Imagine that in our training set the vocabulary word data only occurs in nonspam messages. Then we’d estimate P(data|S) = 0. The result is that our Naive Bayes classifier would always assign spam probability 0 to any message containing the word data, even a message like “data on free bitcoin and authentic rolex watches.” To avoid this problem, we usually use some\n",
        "kind of **smoothing**.\n",
        "\n",
        "In particular, we’ll choose a pseudocount—k—and estimate the probability of seeing the ith word in a spam message as:\n",
        "\n",
        "$ P (X_i|S) = (k + number of spams containing w_i) / (2k + number of spams) $"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "F6DLs1-hG1ON"
      },
      "source": [
        "from typing import Set, NamedTuple, List, Tuple, Dict, Iterable\n",
        "import re\n",
        "import math\n",
        "from collections import defaultdict"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Yjd4ES2WQPBv"
      },
      "source": [
        "def tokenize(text: str) -> Set[str]:\n",
        "  \"\"\"Tokenize the text\n",
        "\n",
        "  This function will first convert the text to lower case,\n",
        "  then it'll find all the words and numbers and then it'll\n",
        "  return the distinct words only.\n",
        "  \"\"\"\n",
        "\n",
        "  text = text.lower()  # Convert to lower case\n",
        "  all_words = re.findall(\"[a-z0-9']+\", text)  # Extract the words\n",
        "  return set(all_words)  # Remove duplicates"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1k3xoCReR3vd"
      },
      "source": [
        "assert tokenize(\"Data Science is Science\") == {\"data\", \"science\", \"is\"}"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CgNHxwQJSJyq"
      },
      "source": [
        "#We'll define a type for our training data\n",
        "class Message(NamedTuple):\n",
        "  text: str\n",
        "  is_spam: bool"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PclEA8FoUOAz"
      },
      "source": [
        "Next, we’ll give it a method to train it on a bunch of messages. First, we increment the spam_messages and ham_messages counts. Then we tokenize each messagetext, and for each token we increment the token_spam_counts or token_ham_counts based on the message type:"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "L02CuNtGXR3R"
      },
      "source": [
        "Ultimately we’ll want to predict P(spam | token). As we saw earlier, to apply\n",
        "Bayes’s theorem we need to know P(token | spam) and P(token | ham) for each\n",
        "token in the vocabulary. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jv_oremfSsxP"
      },
      "source": [
        "# As our classifier needs to keep track of tokens, counts, and labels from the training\n",
        "# data, we’ll make it a class.\n",
        "class NaiveBayesClassifier:\n",
        "  \n",
        "  def __init__(self, k: float=0.5) -> None:\n",
        "    self.k = k  # Smoothening Factor\n",
        "    self.tokens: Set[str] = set()\n",
        "    self.token_spam_counts: Dict[str, int] = defaultdict(int)\n",
        "    self.token_ham_counts: Dict[str, int] = defaultdict(int)\n",
        "    self.spam_messages = self.ham_messages = 0\n",
        "\n",
        "  def train(self, messages: Iterable[Message]) -> None:\n",
        "    for message in messages:\n",
        "      # Increment message counts\n",
        "      if message.is_spam:\n",
        "        self.spam_messages += 1\n",
        "      else:\n",
        "        self.ham_messages += 1\n",
        "      \n",
        "      # Increment word count\n",
        "      for token in tokenize(message.text):\n",
        "        self.tokens.add(token)\n",
        "        if message.is_spam:\n",
        "          self.token_spam_counts[token] += 1\n",
        "        else:\n",
        "          self.token_ham_counts[token] += 1\n",
        "\n",
        "  def fit(self, X: List[str], y: List[int]) -> None:\n",
        "    for message, label in zip(X, y):\n",
        "      # Increment message counts\n",
        "      if label == 1:\n",
        "        self.spam_messages += 1\n",
        "      elif label == 0:\n",
        "        self.ham_messages += 1\n",
        "      \n",
        "      # Increment word count\n",
        "      for token in tokenize(message):\n",
        "        self.tokens.add(token)\n",
        "        if label == 1:\n",
        "          self.token_spam_counts[token] += 1\n",
        "        elif label == 0:\n",
        "          self.token_ham_counts[token] += 1\n",
        "\n",
        "  def _probabilities(self, token: str) -> Tuple[float, float]:\n",
        "    \"\"\"returns P(token|spam) and P(token|ham)\"\"\"\n",
        "    spam = self.token_spam_counts[token]\n",
        "    ham = self.token_ham_counts[token]\n",
        "    p_token_spam = (spam + self.k) / (self.spam_messages + 2 * self.k)\n",
        "    p_token_ham = (ham + self.k) / (self.ham_messages + 2 * self.k)\n",
        "    return p_token_spam, p_token_ham\n",
        "\n",
        "  def predict(self, text: str) -> float:\n",
        "    text_tokens = tokenize(text)\n",
        "    log_prob_if_spam = log_prob_if_ham = 0.0\n",
        "    # Iterate through each word in out vocab\n",
        "    for token in self.tokens:\n",
        "      prob_if_spam, prob_if_ham = self._probabilities(token)\n",
        "      # If token appears in the message\n",
        "      # add the log probability of seeing it\n",
        "      if token in text_tokens:\n",
        "        log_prob_if_spam += math.log(prob_if_spam)\n",
        "        log_prob_if_ham += math.log(prob_if_ham)\n",
        "      # Otherwise add the log probability of _not_ seeing it,\n",
        "      # which is log(1 - probability of seeing it)\n",
        "      else:\n",
        "        log_prob_if_spam += math.log(1.0 - prob_if_spam)\n",
        "        log_prob_if_ham += math.log(1.0 - prob_if_ham)\n",
        "    \n",
        "    prob_if_spam = math.exp(log_prob_if_spam)\n",
        "    prob_if_ham = math.exp(log_prob_if_ham)\n",
        "    return prob_if_spam / (prob_if_spam + prob_if_ham)"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "W1Rt5y6UlCwi"
      },
      "source": [
        "## Testing"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "USyEInkkkDG8"
      },
      "source": [
        "messages = [\n",
        "      Message(\"spam rules\", is_spam=True),\n",
        "      Message(\"ham rules\", is_spam=False),\n",
        "      Message(\"hello ham\", is_spam=False)\n",
        "]\n",
        "model = NaiveBayesClassifier(k=0.5)\n",
        "model.train(messages)"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Cq2A5DlrlW_G",
        "outputId": "edea193b-837a-4ebc-8ad7-ce1ab4e30b70"
      },
      "source": [
        "model.tokens == {\"spam\", \"ham\", \"rules\", \"hello\"}"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gzf6C23pmMKB",
        "outputId": "d1268ec3-b946-4dd3-ffbf-16a397ec1d04"
      },
      "source": [
        "model.spam_messages == 1"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hLxU-0FwmOPW",
        "outputId": "7c21bee6-534b-4e6d-dbfe-4a2cdf05b711"
      },
      "source": [
        "model.ham_messages == 2"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TiT1U0wFmQJ5",
        "outputId": "8e8bc547-85bb-4739-b47a-9084fb64d8fb"
      },
      "source": [
        "model.token_spam_counts == {\"spam\": 1, \"rules\": 1}"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GJR3IqNHmSh2",
        "outputId": "34ed0661-46bf-4f11-9e84-cb99660c20d4"
      },
      "source": [
        "model.token_ham_counts == {\"ham\": 2, \"rules\": 1, \"hello\": 1}"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "x95NpwIxmViA"
      },
      "source": [
        "text = \"hello spam\"\n",
        "probs_if_spam = [\n",
        "    (1 + 0.5) / (1 + 2 * 0.5),  # \"spam\" (present)\n",
        "    1 - (0 + 0.5) / (1 + 2 * 0.5),  # \"ham\" (not present)\n",
        "    1 - (1 + 0.5) / (1 + 2 * 0.5),  # \"rules\" (not present)\n",
        "    (0 + 0.5) / (1 + 2 * 0.5)  # \"hello\" (present)\n",
        "]"
      ],
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ORrKTDVImmhe"
      },
      "source": [
        "probs_if_ham = [\n",
        "(0 + 0.5) / (2 + 2 * 0.5),  # \"spam\" (present)\n",
        "1 - (2 + 0.5) / (2 + 2 * 0.5),  # \"ham\" (not present)\n",
        "1 - (1 + 0.5) / (2 + 2 * 0.5),  # \"rules\" (not present)\n",
        "(1 + 0.5) / (2 + 2 * 0.5),  # \"hello\" (present)\n",
        "]"
      ],
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "reoFXqwUmr5E"
      },
      "source": [
        "p_if_spam = math.exp(sum(math.log(p) for p in probs_if_spam))\n",
        "p_if_ham = math.exp(sum(math.log(p) for p in probs_if_ham))"
      ],
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lrrqS67MnCrI",
        "outputId": "54c9a716-73d5-4af5-ba34-0de34c488abf"
      },
      "source": [
        "model.predict(text)"
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.8350515463917525"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 15
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DEuqvVu9m-3v",
        "outputId": "f7eafce8-0f7d-4770-b85d-88e83b448997"
      },
      "source": [
        "model.predict(text) == p_if_spam / (p_if_spam + p_if_ham)"
      ],
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 16
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ArXTGv9inbF7"
      },
      "source": [
        "This test passes, so it seems like our model is doing what we think it is. If you look\n",
        "at the actual probabilities, the two big drivers are that our message contains spam\n",
        "(which our lone training spam message did) and that it doesn’t contain ham (which\n",
        "both our training ham messages did)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KtGiwbs6neg-"
      },
      "source": [
        "## Using Our Model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ERSP0A9gnhLg"
      },
      "source": [
        "A popular (if somewhat old) dataset is the SpamAssassin public corpus. We’ll look\n",
        "at the files prefixed with 20021010."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9qKNuZfanr2e"
      },
      "source": [
        "from io import BytesIO  # So we can treat bytes as a file.\n",
        "import requests  # To download the files, which\n",
        "import tarfile  # are in .tar.bz format."
      ],
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AurQf9KVn3lN"
      },
      "source": [
        "BASE_URL = \"https://spamassassin.apache.org/old/publiccorpus\"\n",
        "FILES = [\n",
        "    \"20021010_easy_ham.tar.bz2\",\n",
        "    \"20021010_hard_ham.tar.bz2\",\n",
        "    \"20021010_spam.tar.bz2\"\n",
        "]\n",
        "OUTPUT_DIR = 'spam_data'"
      ],
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8fwOGaquoBTD"
      },
      "source": [
        "for file in FILES:\n",
        "  # Use requests to get the file contents at each URL.\n",
        "  content = requests.get(f\"{BASE_URL}/{file}\").content\n",
        "  # Wrap the in-memory bytes so we can use them as a \"file.\"\n",
        "  fin = BytesIO(content)\n",
        "  # And extract all the files to the specified output dir.\n",
        "  with tarfile.open(fileobj=fin, mode='r:bz2') as tf:\n",
        "    tf.extractall(OUTPUT_DIR)"
      ],
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Iw7_Ibb-pl0g"
      },
      "source": [
        "After downloading the data you should have three folders: spam, easy_ham, and hard_ham. Each folder contains many emails, each contained in a single file. To keep things really simple, we’ll just look at the subject lines of each email."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yAUjq6GspEkK",
        "outputId": "f417c656-74d9-4aa0-a970-8e15eb739dc1"
      },
      "source": [
        "! ls spam_data"
      ],
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "easy_ham  hard_ham  spam\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1UkX7m_NpbSA"
      },
      "source": [
        "How do we identify the subject line? When we look through the files, they all seem to start with “Subject:”. So we’ll look for that:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HdUbGkbDpzKh"
      },
      "source": [
        "import glob"
      ],
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GNJM50sxq2qw"
      },
      "source": [
        "path = 'spam_data/*/*'\n",
        "data: List[Message] = []"
      ],
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "is8GURNYrBDj"
      },
      "source": [
        "# glob.glob returns every filename that matches the wildcarded path\n",
        "for filename in glob.glob(path):\n",
        "  # Boolean mask to set is_spam by looking at the filename\n",
        "  # Since the filename contains the word 'spam' or 'ham'\n",
        "  is_spam = \"ham\" not in filename\n",
        "  # There are some garbage characters in the emails; the errors='ignore'\n",
        "  # skips them instead of raising an exception.\n",
        "  with open(filename, errors='ignore') as email_file:\n",
        "    for line in email_file:\n",
        "      if line.startswith(\"Subject:\"):\n",
        "        subject = line.lstrip(\"Subject: \")\n",
        "        data.append(Message(subject, is_spam))\n",
        "        break"
      ],
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "knvDt-hAt9JZ"
      },
      "source": [
        "import random\n",
        "from typing import TypeVar\n",
        "X = TypeVar('X')  # generic type to represent a data point"
      ],
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "taxuzd7hsdXT"
      },
      "source": [
        "def split_data(data: List[X], prob: float) -> Tuple[List[X], List[X]]:\n",
        "    \"\"\"Split data into fractions [prob, 1 - prob]\"\"\"\n",
        "    data = data[:]                    # Make a shallow copy\n",
        "    random.shuffle(data)              # because shuffle modifies the list.\n",
        "    cut = int(len(data) * prob)       # Use prob to find a cutoff\n",
        "    return data[:cut], data[cut:]     # and split the shuffled list there."
      ],
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gnqfLvR_t_Go"
      },
      "source": [
        "random.seed(0)\n",
        "train_messages, test_messages = split_data(data, 0.75)\n",
        "model = NaiveBayesClassifier()\n",
        "model.train(train_messages)"
      ],
      "execution_count": 26,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "woXH6cmDvN2x"
      },
      "source": [
        "Let’s generate some predictions and check how our model does:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GBbKVcJ4vS3-"
      },
      "source": [
        "from collections import Counter"
      ],
      "execution_count": 27,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "W318G6j1vVLU"
      },
      "source": [
        "predictions = [(message, model.predict(message.text)) for message in test_messages]"
      ],
      "execution_count": 28,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "E02u0WVMv78X",
        "outputId": "b91b55ca-cad2-4fa2-e156-41f01dbcac49"
      },
      "source": [
        "# Assume that spam_probability > 0.5 corresponds to spam prediction\n",
        "# and count the combinations of (actual is_spam, predicted is_spam)\n",
        "confusion_matrix = Counter((message.is_spam, spam_probability > 0.5) for message, spam_probability in predictions)\n",
        "print(confusion_matrix)"
      ],
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Counter({(False, False): 675, (True, True): 85, (True, False): 44, (False, True): 21})\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SPUr4i-gxVoV"
      },
      "source": [
        "tn = confusion_matrix[(False, False)]\n",
        "tp = confusion_matrix[(True, True)]\n",
        "fp = confusion_matrix[(False, True)]\n",
        "fn = confusion_matrix[(True, False)]"
      ],
      "execution_count": 30,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2wezeQSxx64b",
        "outputId": "d40c0236-5867-4140-ca6d-e404362f24e5"
      },
      "source": [
        "precision = tp / (tp + fp)\n",
        "recall = tp / (tp + fn)\n",
        "print(f\"Precision: {round(precision * 100, 2)}%\")\n",
        "print(f\"Recall: {round(recall * 100, 2)}%\")"
      ],
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Precision: 80.19%\n",
            "Recall: 65.89%\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "x_bYGNnRyvSM"
      },
      "source": [
        "We can also inspect the model’s innards to see which words are least and most\n",
        "indicative of spam:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PMemaa7OzZR6"
      },
      "source": [
        "def p_spam_given_token(token: str, model: NaiveBayesClassifier) -> float:\n",
        "  # We probably shouldn't call private methods, but it's for a good cause.\n",
        "  prob_if_spam, prob_if_ham = model._probabilities(token)\n",
        "  return prob_if_spam / (prob_if_spam + prob_if_ham)\n",
        "words = sorted(model.tokens, key=lambda t: p_spam_given_token(t, model))"
      ],
      "execution_count": 32,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bjdtfPRHzpVT",
        "outputId": "4bc57ae3-86a9-48d2-bdb8-8cf4fc544339"
      },
      "source": [
        "print(\"spammiest_words\", words[-10:])\n",
        "print(\"hammiest_words\", words[:10])"
      ],
      "execution_count": 33,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "spammiest_words ['norton', 'per', 'mortgage', 'clearance', 'adv', 'sale', 'systemworks', 'only', 'rates', 'money']\n",
            "hammiest_words ['spambayes', 'users', 'razor', 'zzzzteana', 'apt', 'sadev', 'ouch', 'perl', 'bliss', 'selling']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dnIP3Bjf0H6f"
      },
      "source": [
        "How could we get better performance? One obvious way would be to get more data to train on. There are a number of ways to improve the model as well. Here are some possibilities that you might try:\n",
        "\n",
        "* Look at the message content, not just the subject line. You’ll have to be careful how you deal with the message headers.\n",
        "\n",
        "* Our classifier takes into account every word that appears in the training set, even words that appear only once. Modify the classifier to accept an optional min_count threshold and ignore tokens that don’t appear at least that many times.\n",
        "\n",
        "* The tokenizer has no notion of similar words (e.g., cheap and cheapest). Modify the classifier to take an optional stemmer function that converts words to equivalence classes of words. For example, a really simple stemmer function might be:\n",
        "```\n",
        "def drop_final_s(word):\n",
        "  return re.sub(\"s$\", \"\", word)\n",
        "```\n",
        "\n",
        "* Although our features are all of the form “message contains word wi,” there’s no reason why this has to be the case. In our implementation, we could add extra features like “message contains a number” by creating phony tokens like contains:number and modifying the tokenizer to emit them when appropriate."
      ]
    }
  ]
}